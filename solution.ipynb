{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from solvers import steepest_descent, newton, BFGS, DFP\n",
    "from objectives import get_rosenbrock, get_lgt_obj, get_zakharov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common plotting method for all 3 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(dim, results, title, filename):\n",
    "    plt.figure(figsize=(20, 10))  # Increase the size of the plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for label, (function_history, gradient_norms, cumulative_times) in results.items():\n",
    "        plt.plot(function_history[:100], label=f'{label} - Function Value', linewidth=2) \n",
    "    plt.title(f'{title} - Function Value', fontsize=20)\n",
    "    plt.xlabel('Iterations', fontsize=32)\n",
    "    plt.ylabel('Function Value', fontsize=32)\n",
    "    plt.legend(fontsize=25)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for label, (function_history, gradient_norms, cumulative_times) in results.items():\n",
    "        plt.plot(gradient_norms[:100], label=f'{label} - Gradient Norm', linewidth=2)  \n",
    "    plt.title(f'{title} - Gradient Norm', fontsize=20)\n",
    "    plt.xlabel('Iterations', fontsize=32)\n",
    "    plt.ylabel('Gradient Norm', fontsize=32)\n",
    "    plt.legend(fontsize=25)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common method to save comparision in number of iterations for each optimization algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "\n",
    "    data = {\n",
    "        'Algorithm': [],\n",
    "        'Dimension_or_Lambda': [],\n",
    "        'Time': [],\n",
    "        'Final Function Value': [],\n",
    "        'Final Gradient Norm': [],\n",
    "        'Iterations': []\n",
    "    }\n",
    "\n",
    "    # Populate the dictionary with the results\n",
    "    for dim, methods_results in results.items():\n",
    "        for label, (function_history, gradient_norms,cumulative_times) in methods_results.items():\n",
    "            data['Algorithm'].append(label)\n",
    "            data['Dimension_or_Lambda'].append(dim)\n",
    "            data['Time'].append(cumulative_times[-1])\n",
    "            data['Final Function Value'].append(function_history[-1])\n",
    "            data['Final Gradient Norm'].append(gradient_norms[-1])\n",
    "            data['Iterations'].append(len(function_history))\n",
    "        \n",
    "    df_results = pd.DataFrame(data)\n",
    "    df_results.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest Descent done for dim=2\n",
      "Newton done for dim=2\n",
      "BFGS done for dim=2\n",
      "DFP done for dim=2\n",
      "Steepest Descent done for dim=5\n",
      "Newton done for dim=5\n",
      "BFGS done for dim=5\n",
      "DFP done for dim=5\n",
      "Steepest Descent done for dim=10\n",
      "Newton done for dim=10\n",
      "BFGS done for dim=10\n",
      "DFP done for dim=10\n",
      "Steepest Descent done for dim=50\n",
      "Newton done for dim=50\n",
      "BFGS done for dim=50\n",
      "DFP done for dim=50\n"
     ]
    }
   ],
   "source": [
    "dimensions = [2, 5, 10, 50]\n",
    "results = {}\n",
    "\n",
    "for dim in dimensions:\n",
    "    f, gradf, hessf, x0 = get_rosenbrock(dim)\n",
    "    results[dim] = {}\n",
    "\n",
    "    methods = {\n",
    "        'Steepest Descent': steepest_descent,\n",
    "        'Newton': newton,\n",
    "        'BFGS': BFGS,\n",
    "        'DFP': DFP\n",
    "    }\n",
    "\n",
    "    for label, method in methods.items():\n",
    "        if label == 'Newton':\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "        else:\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "        results[dim][label] = (function_history, gradient_norms, cumulative_times)\n",
    "        print(f\"{label} done for dim={dim}\")\n",
    "\n",
    "    plot_results(dim, results[dim], f'Rosenbrock Function Optimization for Dimension {dim}', f'rosenbrock_{dim}.png')\n",
    "save_results_to_csv(results, 'rosenbrock_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Zakharov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steepest Descent done for dim=2\n",
      "Newton done for dim=2\n",
      "BFGS done for dim=2\n",
      "DFP done for dim=2\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "# Iterate over each dimension\n",
    "\n",
    "f, gradf, hessf, x0 = get_zakharov(2)\n",
    "results[0] = {}\n",
    "\n",
    "methods = {\n",
    "    'Steepest Descent': steepest_descent,\n",
    "    'Newton': newton,\n",
    "    'BFGS': BFGS,\n",
    "    'DFP': DFP\n",
    "}\n",
    "\n",
    "for label, method in methods.items():\n",
    "    if label == 'Newton':\n",
    "        xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "    else:\n",
    "        xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "    results[0][label] = (function_history, gradient_norms, cumulative_times)\n",
    "    print(f\"{label} done for dim={2}\")\n",
    "\n",
    "plot_results(2, results[0], f'Zakharov Function Optimization for Dimension {2}', f'zakharov_2.png')\n",
    "save_results_to_csv(results, 'zakharov_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Steepest Descent for λ=0.001\n",
      "Steepest Descent done for λ=0.001 in 2.600860118865967 seconds in cumulative time 183\n",
      "Running Newton for λ=0.001\n",
      "Newton done for λ=0.001 in 8.772995710372925 seconds in cumulative time 183\n",
      "Running BFGS for λ=0.001\n",
      "BFGS done for λ=0.001 in 14.251598834991455 seconds in cumulative time 409\n",
      "Running DFP for λ=0.001\n",
      "DFP done for λ=0.001 in 12.488960981369019 seconds in cumulative time 720\n",
      "Running Steepest Descent for λ=0.01\n",
      "Steepest Descent done for λ=0.01 in 2.861194133758545 seconds in cumulative time 248\n",
      "Running Newton for λ=0.01\n",
      "Newton done for λ=0.01 in 11.562780141830444 seconds in cumulative time 248\n",
      "Running BFGS for λ=0.01\n",
      "BFGS done for λ=0.01 in 10.199692249298096 seconds in cumulative time 304\n",
      "Running DFP for λ=0.01\n",
      "DFP done for λ=0.01 in 5.242429971694946 seconds in cumulative time 309\n",
      "Running Steepest Descent for λ=0.1\n",
      "Steepest Descent done for λ=0.1 in 2.0232250690460205 seconds in cumulative time 219\n",
      "Running Newton for λ=0.1\n",
      "Newton done for λ=0.1 in 14.808140993118286 seconds in cumulative time 302\n",
      "Running BFGS for λ=0.1\n",
      "BFGS done for λ=0.1 in 4.205564022064209 seconds in cumulative time 128\n",
      "Running DFP for λ=0.1\n",
      "DFP done for λ=0.1 in 2.0977649688720703 seconds in cumulative time 148\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "lambdas = [0.001, 0.01, 0.1]\n",
    "results = {}\n",
    "\n",
    "for lam in lambdas:\n",
    "    f, gradf, hessf, x0 = get_lgt_obj(lam)\n",
    "    results[lam] = {}\n",
    "\n",
    "    methods = {\n",
    "        'Steepest Descent': lambda x0, f, gradf: steepest_descent(x0, f, gradf, c0=0.001, c1=0.9, t0=1e-3, grad_tol=1e-4),\n",
    "        'Newton': lambda x0, f, gradf, hessf: newton(x0, f, gradf, hessf, c0=0.001, c1=0.9, t0=1e-3, grad_tol=1e-4),\n",
    "        'BFGS': lambda x0, f, gradf: BFGS(x0, f, gradf, c0=0.001, c1=0.8, t0=1e-3, grad_tol=1e-3),\n",
    "        'DFP': lambda x0, f, gradf: DFP(x0, f, gradf, c0=0.001, c1=0.5, t0=1e-3, grad_tol=1e-3)\n",
    "    }\n",
    "\n",
    "    for label, method in methods.items():\n",
    "        print(f\"Running {label} for λ={lam}\")\n",
    "        start_time = time.time()\n",
    "        if label == 'Newton':\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "        else:\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "        results[lam][label] = (function_history, gradient_norms, cumulative_times)\n",
    "        end_time = time.time()\n",
    "        print(f\"{label} done for λ={lam} in {end_time - start_time} seconds in cumulative time {cumulative_times[-1]}\")\n",
    "\n",
    "    plot_results(lam, results[lam], f'LGT Function Optimization with λ={lam}', f'lgt_{lam}.png')\n",
    "    \n",
    "save_results_to_csv(results, 'lgt_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning c0 and c1 - simply for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton done for c1=0.5 in 8.88641619682312 seconds\n",
      "Iterations: 164, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "BFGS done for c1=0.5 in 6.263967990875244 seconds\n",
      "Iterations: 172, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "DFP done for c1=0.5 in 18.34773874282837 seconds\n",
      "Iterations: 912, Final Function Value: 0.03127, Final Gradient Norm: 9e-05\n",
      "Newton done for c1=0.7 in 6.870253086090088 seconds\n",
      "Iterations: 143, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "BFGS done for c1=0.7 in 17.31073808670044 seconds\n",
      "Iterations: 423, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "DFP done for c1=0.7 in 104.17127990722656 seconds\n",
      "Iterations: 896, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "Newton done for c1=0.9 in 9.158504962921143 seconds\n",
      "Iterations: 184, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "BFGS done for c1=0.9 in 45.334630727767944 seconds\n",
      "Iterations: 1396, Final Function Value: 0.03127, Final Gradient Norm: 0.0001\n",
      "DFP done for c1=0.9 in 70.66062426567078 seconds\n",
      "Iterations: 1172, Final Function Value: 0.03127, Final Gradient Norm: 9e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define search grid\n",
    "c1_values = [0.5, 0.7, 0.9]\n",
    "\n",
    "# Fixed parameters\n",
    "c0_fixed = 0.001  # Fixed c0 value\n",
    "lambda_fixed = 0.001  # Regularization parameter\n",
    "t0_fixed = 1e-3  # Small step size\n",
    "grad_tol_fixed = 1e-4  # Small enough tolerance\n",
    "\n",
    "# Load LGT problem\n",
    "f, gradf, hessf, x0 = get_lgt_obj(lambda_fixed)\n",
    "\n",
    "# Storage for results\n",
    "results = {}\n",
    "\n",
    "for c1 in c1_values:\n",
    "    # print(i,c1)\n",
    "    results[c1] = {}\n",
    "\n",
    "    methods = {\n",
    "        # 'Steepest Descent': lambda x, f, gradf: steepest_descent(x, f, gradf, c0=c0_fixed, c1=c1, t0=t0_fixed, grad_tol=grad_tol_fixed),\n",
    "        'Newton': lambda x, f, gradf, hessf: newton(x, f, gradf, hessf, c0=c0_fixed, c1=c1, t0=t0_fixed, grad_tol=grad_tol_fixed),\n",
    "        'BFGS': lambda x, f, gradf: BFGS(x, f, gradf, c0=c0_fixed, c1=c1, t0=t0_fixed, grad_tol=grad_tol_fixed),\n",
    "        'DFP': lambda x, f, gradf: DFP(x, f, gradf, c0=c0_fixed, c1=(c1 - 0.4), t0=t0_fixed, grad_tol=grad_tol_fixed)\n",
    "    }\n",
    "\n",
    "    for label, method in methods.items():\n",
    "        start_time = time.time()\n",
    "        if label == 'Newton':\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "        else:\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "        end_time = time.time()\n",
    "\n",
    "        results[c1][label] = {\n",
    "            \"iterations\": len(function_history),\n",
    "            \"final_f\": round(function_history[-1], 5),\n",
    "            \"final_grad_norm\": round(gradient_norms[-1], 5),\n",
    "            \"time\": cumulative_times[-1]\n",
    "        }\n",
    "\n",
    "        print(f\"{label} done for c1={c1} in {end_time - start_time} seconds\")\n",
    "        print(f\"Iterations: {len(function_history)}, Final Function Value: {round(function_history[-1], 5)}, Final Gradient Norm: {round(gradient_norms[-1], 5)}\")\n",
    "\n",
    "# Convert to a heatmap-friendly format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for c0, methods in results.items():\n",
    "    for method, values in methods.items():\n",
    "        data.append([method, c0, values[\"iterations\"], values[\"final_f\"], values[\"final_grad_norm\"]])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Method\", \"c1\", \"Iterations\", \"Final Function Value\", \"Final Gradient Norm\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store df from previous cell to csv\n",
    "# Store df from previous cell to csv\n",
    "df.to_csv('lgt_hyperparam_tuning.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
