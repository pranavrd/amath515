{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from solvers import steepest_descent, newton, BFGS, DFP\n",
    "from objectives import get_rosenbrock, get_lgt_obj, get_zakharov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common plotting method for all 3 functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(dim, results, title, filename):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for label, (function_history, gradient_norms, cumulative_times) in results.items():\n",
    "        plt.plot(function_history[:100], label=f'{label} - Function Value') \n",
    "    plt.title(f'{title} - Function Value')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Function Value')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for label, (function_history, gradient_norms, cumulative_times) in results.items():\n",
    "        plt.plot(gradient_norms[:100], label=f'{label} - Gradient Norm')  \n",
    "    plt.title(f'{title} - Gradient Norm')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common method to save comparision in number of iterations for each optimization algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_results_to_csv(results, filename):\n",
    "\n",
    "    data = {\n",
    "        'Algorithm': [],\n",
    "        'Dimension_or_Lambda': [],\n",
    "        'Time': [],\n",
    "        'Final Function Value': [],\n",
    "        'Final Gradient Norm': [],\n",
    "        'Iterations': []\n",
    "    }\n",
    "\n",
    "    # Populate the dictionary with the results\n",
    "    for dim, methods_results in results.items():\n",
    "        for label, (function_history, gradient_norms,cumulative_times) in methods_results.items():\n",
    "            data['Algorithm'].append(label)\n",
    "            data['Dimension_or_Lambda'].append(dim)\n",
    "            data['Time'].append(cumulative_times[-1])\n",
    "            data['Final Function Value'].append(function_history[-1])\n",
    "            data['Final Gradient Norm'].append(gradient_norms[-1])\n",
    "            data['Iterations'].append(len(function_history))\n",
    "        \n",
    "    df_results = pd.DataFrame(data)\n",
    "    df_results.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Rosebrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [2, 5, 10, 50]\n",
    "results = {}\n",
    "\n",
    "for dim in dimensions:\n",
    "    f, gradf, hessf, x0 = get_rosenbrock(dim)\n",
    "    results[dim] = {}\n",
    "\n",
    "    methods = {\n",
    "        'Steepest Descent': steepest_descent,\n",
    "        'Newton': newton,\n",
    "        'BFGS': BFGS,\n",
    "        'DFP': DFP\n",
    "    }\n",
    "\n",
    "    for label, method in methods.items():\n",
    "        if label == 'Newton':\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "        else:\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "        results[dim][label] = (function_history, gradient_norms, cumulative_times)\n",
    "        print(f\"{label} done for dim={dim}\")\n",
    "\n",
    "    plot_results(dim, results[dim], f'Rosenbrock Function Optimization for Dimension {dim}', f'rosenbrock_{dim}.png')\n",
    "save_results_to_csv(results, 'rosenbrock_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Zakharov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "# Iterate over each dimension\n",
    "\n",
    "f, gradf, hessf, x0 = get_zakharov(2)\n",
    "results[0] = {}\n",
    "\n",
    "methods = {\n",
    "    'Steepest Descent': steepest_descent,\n",
    "    'Newton': newton,\n",
    "    'BFGS': BFGS,\n",
    "    'DFP': DFP\n",
    "}\n",
    "\n",
    "for label, method in methods.items():\n",
    "    if label == 'Newton':\n",
    "        xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "    else:\n",
    "        xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "    results[0][label] = (function_history, gradient_norms, cumulative_times)\n",
    "    print(f\"{label} done for dim={2}\")\n",
    "\n",
    "plot_results(2, results[0], f'Zakharov Function Optimization for Dimension {2}', f'zakharov_2.png')\n",
    "save_results_to_csv(results, 'zakharov_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Logistic Regression on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Steepest Descent for λ=0.001\n",
      "Steepest Descent done for λ=0.001 in 30.302498817443848 seconds in cumulative time 226\n",
      "Running Newton for λ=0.001\n",
      "Newton done for λ=0.001 in 9.94852590560913 seconds in cumulative time 183\n",
      "Running BFGS for λ=0.001\n",
      "BFGS done for λ=0.001 in 15.671884059906006 seconds in cumulative time 409\n",
      "Running DFP for λ=0.001\n",
      "DFP done for λ=0.001 in 14.879886150360107 seconds in cumulative time 720\n",
      "Running Steepest Descent for λ=0.01\n",
      "Steepest Descent done for λ=0.01 in 20.86095690727234 seconds in cumulative time 237\n",
      "Running Newton for λ=0.01\n",
      "Newton done for λ=0.01 in 13.015438795089722 seconds in cumulative time 248\n",
      "Running BFGS for λ=0.01\n",
      "BFGS done for λ=0.01 in 11.680152654647827 seconds in cumulative time 304\n",
      "Running DFP for λ=0.01\n",
      "DFP done for λ=0.01 in 6.108936071395874 seconds in cumulative time 309\n",
      "Running Steepest Descent for λ=0.1\n",
      "Steepest Descent done for λ=0.1 in 4.841114044189453 seconds in cumulative time 152\n",
      "Running Newton for λ=0.1\n",
      "Newton done for λ=0.1 in 15.330960988998413 seconds in cumulative time 302\n",
      "Running BFGS for λ=0.1\n",
      "BFGS done for λ=0.1 in 5.019186973571777 seconds in cumulative time 128\n",
      "Running DFP for λ=0.1\n",
      "DFP done for λ=0.1 in 2.784335136413574 seconds in cumulative time 148\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "lambdas = [0.001, 0.01, 0.1]\n",
    "results = {}\n",
    "\n",
    "for lam in lambdas:\n",
    "    f, gradf, hessf, x0 = get_lgt_obj(lam)\n",
    "    results[lam] = {}\n",
    "\n",
    "    methods = {\n",
    "        'Steepest Descent': lambda x0, f, gradf: steepest_descent(x0, f, gradf, c0=0.001, c1=0.9, t0=0.1e-3, grad_tol=1e-4),\n",
    "        'Newton': lambda x0, f, gradf, hessf: newton(x0, f, gradf, hessf, c0=0.001, c1=0.9, t0=1e-3, grad_tol=1e-4),\n",
    "        'BFGS': lambda x0, f, gradf: BFGS(x0, f, gradf, c0=0.001, c1=0.8, t0=1e-3, grad_tol=1e-3),\n",
    "        'DFP': lambda x0, f, gradf: DFP(x0, f, gradf, c0=0.001, c1=0.5, t0=1e-3, grad_tol=1e-3)\n",
    "    }\n",
    "\n",
    "    for label, method in methods.items():\n",
    "        print(f\"Running {label} for λ={lam}\")\n",
    "        start_time = time.time()\n",
    "        if label == 'Newton':\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "        else:\n",
    "            xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf)\n",
    "        results[lam][label] = (function_history, gradient_norms, cumulative_times)\n",
    "        end_time = time.time()\n",
    "        print(f\"{label} done for λ={lam} in {end_time - start_time} seconds in cumulative time {cumulative_times[-1]}\")\n",
    "\n",
    "    plot_results(lam, results[lam], f'LGT Function Optimization with λ={lam}', f'lgt_{lam}.png')\n",
    "    \n",
    "save_results_to_csv(results, 'lgt_results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning c0 and c1 - simply for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton done for c0=0.0001 in 9.11731505393982 seconds\n",
      "Iterations: 184, Final Function Value: 0.031271444413236635, Final Gradient Norm: 9.652822419282265e-05\n",
      "Newton done for c0=0.001 in 10.520976066589355 seconds\n",
      "Iterations: 184, Final Function Value: 0.031271444413236635, Final Gradient Norm: 9.652822419282265e-05\n",
      "Newton done for c0=0.01 in 11.210922002792358 seconds\n",
      "Iterations: 184, Final Function Value: 0.031271444413236635, Final Gradient Norm: 9.652822419282265e-05\n",
      "   Method      c0  Iterations  Final Function Value  Final Gradient Norm\n",
      "0  Newton  0.0001         184              0.031271             0.000097\n",
      "1  Newton  0.0010         184              0.031271             0.000097\n",
      "2  Newton  0.0100         184              0.031271             0.000097\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Define search grid\n",
    "c0_values = [1e-4, 1e-3, 1e-2]  # Typical range\n",
    "\n",
    "# Fixed parameters\n",
    "c1_fixed = 0.9  # Fixed c1 value\n",
    "lambda_fixed = 0.001  # Regularization parameter\n",
    "t0_fixed = 1e-3  # Small step size\n",
    "grad_tol_fixed = 1e-4  # Small enough tolerance\n",
    "\n",
    "# Load LGT problem\n",
    "f, gradf, hessf, x0 = get_lgt_obj(lambda_fixed)\n",
    "\n",
    "# Storage for results\n",
    "results = {}\n",
    "\n",
    "for c0 in c0_values:\n",
    "    results[c0] = {}\n",
    "\n",
    "    # Test Newton method\n",
    "    method = lambda x, f, gradf, hessf: newton(x, f, gradf, hessf, c0=c0, c1=c1_fixed, t0=t0_fixed, grad_tol=grad_tol_fixed)\n",
    "\n",
    "    start_time = time.time()\n",
    "    xsol, function_history, cumulative_times, gradient_norms = method(x0, f, gradf, hessf)\n",
    "    end_time = time.time()\n",
    "\n",
    "    results[c0][\"Newton\"] = {\n",
    "        \"iterations\": len(function_history),\n",
    "        \"final_f\": function_history[-1],\n",
    "        \"final_grad_norm\": gradient_norms[-1]\n",
    "    }\n",
    "\n",
    "    print(f\"Newton done for c0={c0} in {end_time - start_time} seconds\")\n",
    "    print(f\"Iterations: {len(function_history)}, Final Function Value: {function_history[-1]}, Final Gradient Norm: {gradient_norms[-1]}\")\n",
    "\n",
    "# Convert to a heatmap-friendly format\n",
    "data = []\n",
    "for c0, methods in results.items():\n",
    "    for method, values in methods.items():\n",
    "        data.append([method, c0, values[\"iterations\"], values[\"final_f\"], values[\"final_grad_norm\"]])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Method\", \"c0\", \"Iterations\", \"Final Function Value\", \"Final Gradient Norm\"])\n",
    "\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
